# ğŸš€ AMD ROCm è’¸é¦è®­ç»ƒå®Œæ•´æ–¹æ¡ˆ

**è®¾å¤‡ï¼š** æ‘©å°”çº¿ç¨‹ M1A Pro+ï¼ˆRadeon 8060S, 96GB æ˜¾å­˜ï¼‰
**ç³»ç»Ÿï¼š** Windows 11 â†’ WSL2 + Ubuntu 22.04
**ç›®æ ‡ï¼š** åˆ›å»ºå¯æ§åˆ¶çš„è’¸é¦è®­ç»ƒæœåŠ¡ï¼ˆæœ‰ç©ºè¿è¡Œ/æ²¡ç©ºå…³é—­ï¼‰
**å®Œæˆæ—¶é—´ï¼š** 2026-02-18

---

## ğŸ“‹ å®Œæ•´æ–¹æ¡ˆæ€»ç»“

| é¡¹ç›® | æ–¹æ¡ˆ |
|------|------|
| **æ“ä½œç³»ç»Ÿ** | WSL2 + Ubuntu 22.04 |
| **GPU é©±åŠ¨** | ROCm 6.0+ |
| **è®­ç»ƒæ¡†æ¶** | LLaMA-Factoryï¼ˆROCm æ”¯æŒæœ€å¥½ï¼‰ |
| **æœåŠ¡ç®¡ç†** | bash è„šæœ¬ + tmux |
| **æ§åˆ¶å‘½ä»¤** | start/stop/status.sh |
| **æ–­ç‚¹ç»­è®­** | æ¯ 500 æ­¥è‡ªåŠ¨ä¿å­˜ checkpoint |
| **èµ„æºç›‘æ§** | rocm-smi + è‡ªå®šä¹‰è„šæœ¬ |
| **æ¨ç†å¼•æ“** | llama.cpp / Ollamaï¼ˆå¾…ç¡®è®¤ ROCm æ”¯æŒï¼‰ |
| **Claw é›†æˆ** | æœ¬åœ° HTTP API ç«¯ç‚¹ |

---

## ğŸ”§ ç¬¬ 1 æ­¥ï¼šWSL2 å®‰è£…ï¼ˆ10 åˆ†é’Ÿï¼‰

```bash
# 1. å¯ç”¨ WSL2ï¼ˆWindows ç®¡ç†å‘˜ PowerShellï¼‰
wsl --install -d Ubuntu-22.04

# 2. è®¾ç½® WSL2 ä¸ºé»˜è®¤ç‰ˆæœ¬
wsl --set-default-version 2

# 3. éªŒè¯å®‰è£…
wsl --list --verbose

# 4. è¿›å…¥ WSL2
wsl -d Ubuntu-22.04
```

**éªŒè¯ï¼š**
```bash
# åœ¨ WSL2 ä¸­æ‰§è¡Œ
uname -a  # åº”æ˜¾ç¤º Linux å†…æ ¸
```

---

## ğŸ”§ ç¬¬ 2 æ­¥ï¼šROCm 6.0 å®‰è£…ï¼ˆ30 åˆ†é’Ÿï¼‰

```bash
# 1. æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# 2. å®‰è£…å¿…è¦ä¾èµ–
sudo apt install -y wget gnupg2 software-properties-common

# 3. æ·»åŠ  ROCm ä»“åº“
wget https://repo.radeon.com/rocm/rocm.gpg.key
gpg --dearmor rocm.gpg.key
sudo mv rocm.gpg.key /etc/apt/trusted.gpg.d/
sudo add-apt-repository 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/6.0 jammy main'

# 4. å®‰è£… ROCm
sudo apt update
sudo apt install -y rocm-dkms rocm-smi-lib rocm-opencl-runtime

# 5. æ·»åŠ ç”¨æˆ·åˆ° video å’Œ render ç»„
sudo usermod -aG video $USER
sudo usermod -aG render $USER

# 6. è®¾ç½®ç¯å¢ƒå˜é‡
echo 'export PATH=/opt/rocm/bin:$PATH' >> ~/.bashrc
echo 'export HSA_OVERRIDE_GFX_VERSION=11.0.0' >> ~/.bashrc
source ~/.bashrc

# 7. éªŒè¯ ROCm å®‰è£…
rocm-smi --showall
```

**é¢„æœŸè¾“å‡ºï¼š** æ˜¾ç¤º GPU ä¿¡æ¯ã€æ¸©åº¦ã€æ˜¾å­˜ç­‰

---

## ğŸ”§ ç¬¬ 3 æ­¥ï¼šPython ç¯å¢ƒ + LLaMA-Factory å®‰è£…ï¼ˆ20 åˆ†é’Ÿï¼‰

```bash
# 1. å®‰è£… Python 3.10+
sudo apt install -y python3 python3-pip python3-venv

# 2. åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/amd-rocm-training
cd ~/amd-rocm-training

# 3. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv .venv
source .venv/bin/activate

# 4. å®‰è£… PyTorch ROCm ç‰ˆæœ¬
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0

# 5. éªŒè¯ PyTorch è¯†åˆ« GPU
python3 -c "import torch; print(f'CUDA å¯ç”¨ï¼š{torch.cuda.is_available()}'); print(f'GPU æ•°é‡ï¼š{torch.cuda.device_count()}')"

# 6. å®‰è£… LLaMA-Factory
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .[torch,metrics]

# 7. éªŒè¯å®‰è£…
llamafactory-cli version
```

---

## ğŸ”§ ç¬¬ 4 æ­¥ï¼šåˆ›å»ºè®­ç»ƒè„šæœ¬ï¼ˆ15 åˆ†é’Ÿï¼‰

### ç›®å½•ç»“æ„
```bash
cd ~/amd-rocm-training
mkdir -p scripts config checkpoints logs
```

### training_config.yaml
```yaml
# config/training_config.yaml
model_name_or_path: Qwen/Qwen2.5-7B-Instruct
adapter_name_or_path: null
template: qwen
finetuning_type: lora
lora_target: all

# æ•°æ®é›†
dataset: alpaca_en_demo  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†
dataset_dir: ../data
cutoff_len: 2048
preprocessing_num_workers: 4

# è®­ç»ƒå‚æ•°
output_dir: ./checkpoints
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
fp16: true  # ROCm æ”¯æŒ FP16

# Checkpoint ä¿å­˜
save_strategy: steps
save_steps: 500
save_total_limit: 3
save_safetensors: true

# æ–­ç‚¹ç»­è®­
resume_from_checkpoint: true
load_best_model_at_end: true

# æ—¥å¿—
logging_dir: ../logs
logging_steps: 10
report_to: none
```

### start_training.sh
```bash
#!/bin/bash
set -e

TRAINING_DIR="$HOME/amd-rocm-training"
CHECKPOINT_DIR="$TRAINING_DIR/checkpoints"
LOG_DIR="$TRAINING_DIR/logs"
TMUX_SESSION="rocm_training"

# åˆ›å»ºç›®å½•
mkdir -p "$CHECKPOINT_DIR" "$LOG_DIR"

# æ£€æŸ¥æ˜¯å¦å·²æœ‰è¿è¡Œä¼šè¯
if tmux has-session -t $TMUX_SESSION 2>/dev/null; then
  echo "âš ï¸ è®­ç»ƒä¼šè¯å·²åœ¨è¿è¡Œä¸­"
  tmux attach -t $TMUX_SESSION
  exit 0
fi

# å¯åŠ¨è®­ç»ƒä¼šè¯
echo "ğŸš€ å¯åŠ¨ ROCm è®­ç»ƒä¼šè¯..."
tmux new-session -d -s $TMUX_SESSION

# åœ¨ä¼šè¯ä¸­è¿è¡Œè®­ç»ƒ
tmux send-keys -t $TMUX_SESSION "cd $TRAINING_DIR/LLaMA-Factory" C-m
tmux send-keys -t $TMUX_SESSION "source ../.venv/bin/activate" C-m
tmux send-keys -t $TMUX_SESSION "export HSA_OVERRIDE_GFX_VERSION=11.0.0" C-m
tmux send-keys -t $TMUX_SESSION "python -m llamafactory.cli train ../config/training_config.yaml 2>&1 | tee ../logs/training.log" C-m

# é™„åŠ åˆ°ä¼šè¯
tmux attach -t $TMUX_SESSION
```

### stop_training.sh
```bash
#!/bin/bash

TMUX_SESSION="rocm_training"

if ! tmux has-session -t $TMUX_SESSION 2>/dev/null; then
  echo "âŒ è®­ç»ƒä¼šè¯æœªè¿è¡Œ"
  exit 1
fi

echo "ğŸ›‘ ä¼˜é›…åœæ­¢è®­ç»ƒ..."

# å‘é€ Ctrl+C ä¿¡å·
tmux send-keys -t $TMUX_SESSION C-c

# ç­‰å¾… 5 ç§’è®© checkpoint ä¿å­˜
sleep 5

# æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
if tmux has-session -t $TMUX_SESSION 2>/dev/null; then
  echo "âš ï¸ å¼ºåˆ¶ç»ˆæ­¢ä¼šè¯..."
  tmux kill-session -t $TMUX_SESSION
fi

echo "âœ… è®­ç»ƒå·²åœæ­¢"
```

### status.sh
```bash
#!/bin/bash

TMUX_SESSION="rocm_training"

echo "=== ğŸ“Š ROCm è®­ç»ƒçŠ¶æ€ ==="
echo ""

# æ£€æŸ¥ tmux ä¼šè¯
if tmux has-session -t $TMUX_SESSION 2>/dev/null; then
  echo "âœ… è®­ç»ƒä¼šè¯ï¼šè¿è¡Œä¸­"
  echo "  ä¼šè¯åï¼š$TMUX_SESSION"
  echo "  é™„åŠ å‘½ä»¤ï¼štmux attach -t $TMUX_SESSION"
else
  echo "âŒ è®­ç»ƒä¼šè¯ï¼šæœªè¿è¡Œ"
fi

echo ""
echo "=== ğŸ–¥ï¸ GPU çŠ¶æ€ ==="
rocm-smi --showall 2>/dev/null || echo "âš ï¸ rocm-smi ä¸å¯ç”¨"

echo ""
echo "=== ğŸ’¾ æœ€æ–° Checkpoint ==="
CHECKPOINT_DIR="$HOME/amd-rocm-training/checkpoints"
if [ -d "$CHECKPOINT_DIR" ]; then
  ls -lt "$CHECKPOINT_DIR" | head -5
else
  echo "æ—  checkpoint ç›®å½•"
fi

echo ""
echo "=== ğŸ“ æœ€æ–°æ—¥å¿— ==="
LOG_DIR="$HOME/amd-rocm-training/logs"
if [ -f "$LOG_DIR/training.log" ]; then
  tail -20 "$LOG_DIR/training.log"
fi
```

### èµ‹äºˆæ‰§è¡Œæƒé™
```bash
chmod +x scripts/*.sh
```

---

## ğŸ”§ ç¬¬ 5 æ­¥ï¼šèµ„æºç›‘æ§ï¼ˆ10 åˆ†é’Ÿï¼‰

### monitor.sh
```bash
#!/bin/bash

echo "=== ğŸ” ROCm GPU ç›‘æ§ ==="
echo "æ—¶é—´ï¼š$(date)"
echo ""

# GPU çŠ¶æ€
echo "ğŸ“Š GPU ä½¿ç”¨ç‡ï¼š"
rocm-smi --showuse

echo ""
echo "ğŸŒ¡ï¸ æ¸©åº¦ï¼š"
rocm-smi --showtemp

echo ""
echo "ğŸ’¾ æ˜¾å­˜ä½¿ç”¨ï¼š"
rocm-smi --showmeminfo vram

echo ""
echo "âš¡ åŠŸè€—ï¼š"
rocm-smi --showpower

# å‘Šè­¦æ£€æŸ¥
TEMP=$(rocm-smi --showtemp | grep -oP '\d+' | head -1)
if [ "$TEMP" -gt 85 ]; then
  echo ""
  echo "ğŸš¨ è­¦å‘Šï¼šGPU æ¸©åº¦è¿‡é«˜ ($TEMPÂ°C)"
  echo "  å»ºè®®ï¼šé™ä½ batch_size æˆ–å¢åŠ å†·å´"
fi

VRAM_USED=$(rocm-smi --showmeminfo vram | grep -oP '\d+' | head -1)
VRAM_TOTAL=96  # 96GB
VRAM_PERCENT=$((VRAM_USED * 100 / VRAM_TOTAL))

if [ "$VRAM_PERCENT" -gt 90 ]; then
  echo ""
  echo "ğŸš¨ è­¦å‘Šï¼šæ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜ ($VRAM_PERCENT%)"
  echo "  å»ºè®®ï¼šé™ä½ batch_size æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯"
fi
```

### ä½¿ç”¨æ–¹å¼
```bash
# æ‰‹åŠ¨ç›‘æ§
./scripts/monitor.sh

# æˆ–å®šæœŸç›‘æ§ï¼ˆæ¯ 60 ç§’ï¼‰
watch -n 60 './scripts/monitor.sh'
```

---

## ğŸ”§ ç¬¬ 6 æ­¥ï¼šæ¨¡å‹å¯¼å‡º + Claw é›†æˆï¼ˆå¾…éªŒè¯ï¼‰

### æ–¹æ¡ˆ Aï¼šllama.cppï¼ˆæ¨èï¼‰
```bash
# 1. å®‰è£… llama.cpp
cd ~
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make -j

# 2. å¯¼å‡ºæ¨¡å‹ä¸º GGUF
python convert-hf-to-gguf.py ~/amd-rocm-training/checkpoints/best_model/ --outfile qwen2.5-7b-distilled.gguf

# 3. è¿è¡Œæ¨ç†
./server -m qwen2.5-7b-distilled.gguf --host 0.0.0.0 --port 8080
```

### æ–¹æ¡ˆ Bï¼šOllamaï¼ˆéœ€éªŒè¯ ROCm æ”¯æŒï¼‰
```bash
# 1. å®‰è£… Ollamaï¼ˆLinux ç‰ˆæœ¬ï¼‰
curl -fsSL https://ollama.com/install.sh | sh

# 2. åˆ›å»º Modelfile
echo "FROM ./qwen2.5-7b-distilled.gguf" > Modelfile
echo "PARAMETER temperature 0.7" >> Modelfile

# 3. å¯¼å…¥æ¨¡å‹
ollama create qwen2.5-7b-distilled -f Modelfile

# 4. è¿è¡Œ
ollama run qwen2.5-7b-distilled
```

### Claw é…ç½®ï¼ˆç¤ºä¾‹ï¼‰
```yaml
# claw_config.yaml
models:
  primary:
    provider: nvidia
    api_key: ${NVIDIA_API_KEY}
    model: llama-3.1-70b-instruct
  
  fallback:
    provider: local
    endpoint: http://localhost:8080  # llama.cpp server
    model: qwen2.5-7b-distilled
  
  switch_policy:
    - on_api_error
    - on_rate_limit
    - manual_switch
```

---

## âš ï¸ é£é™©æ¸…å• + åº”å¯¹æªæ–½

| é£é™© | æ¦‚ç‡ | å½±å“ | åº”å¯¹æªæ–½ |
|------|------|------|----------|
| **ROCm 6.0 ä¸æ”¯æŒ RDNA 3.5** | ä¸­ | é«˜ | å°è¯• ROCm 6.1+ æˆ–ç­‰å¾…å®˜æ–¹æ”¯æŒ |
| **WSL2 GPU ç›´é€šå¤±è´¥** | ä½ | é«˜ | ä½¿ç”¨åŒç³»ç»Ÿ Linux |
| **è®­ç»ƒ OOM** | ä¸­ | ä¸­ | é™ä½ batch_sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯ |
| **è®­ç»ƒä¸­æ–­** | ä¸­ | ä¸­ | Checkpoint æ¯ 500 æ­¥ä¿å­˜ï¼Œè‡ªåŠ¨æ¢å¤ |
| **llama.cpp ROCm æ”¯æŒé—®é¢˜** | ä¸­ | ä¸­ | ä½¿ç”¨ CPU æ¨ç†ï¼ˆæ…¢ä½†å¯ç”¨ï¼‰ |
| **Claw é›†æˆå¤±è´¥** | ä½ | é«˜ | å‡†å¤‡ç‹¬ç«‹ CLI å·¥å…·ä½œä¸ºå¤‡é€‰ |

---

## ğŸ“Š å®Œæ•´å®æ–½æ—¶é—´è¡¨

| æ­¥éª¤ | ä»»åŠ¡ | é¢„è®¡æ—¶é—´ | çŠ¶æ€ |
|------|------|----------|------|
| **1** | WSL2 å®‰è£… | 10 åˆ†é’Ÿ | â³ å¾…æ‰§è¡Œ |
| **2** | ROCm 6.0 å®‰è£… | 30 åˆ†é’Ÿ | â³ å¾…æ‰§è¡Œ |
| **3** | Python + LLaMA-Factory | 20 åˆ†é’Ÿ | â³ å¾…æ‰§è¡Œ |
| **4** | åˆ›å»ºè®­ç»ƒè„šæœ¬ | 15 åˆ†é’Ÿ | â³ å¾…æ‰§è¡Œ |
| **5** | èµ„æºç›‘æ§é…ç½® | 10 åˆ†é’Ÿ | â³ å¾…æ‰§è¡Œ |
| **6** | æ¨¡å‹å¯¼å‡º + Claw é›†æˆ | 30 åˆ†é’Ÿ | â³ å¾…æ‰§è¡Œ |
| **æ€»è®¡** | | **~2 å°æ—¶** | |

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨å‘½ä»¤æ¸…å•

```bash
# === Windows ç«¯ï¼ˆç®¡ç†å‘˜ PowerShellï¼‰ ===
wsl --install -d Ubuntu-22.04
wsl --set-default-version 2

# === WSL2 ç«¯ï¼ˆUbuntu ç»ˆç«¯ï¼‰ ===
# 1. å®‰è£… ROCm
sudo apt update && sudo apt upgrade -y
sudo apt install -y wget gnupg2 software-properties-common
wget https://repo.radeon.com/rocm/rocm.gpg.key
gpg --dearmor rocm.gpg.key
sudo mv rocm.gpg.key /etc/apt/trusted.gpg.d/
sudo add-apt-repository 'deb [arch=amd64] https://repo.radeon.com/rocm/apt/6.0 jammy main'
sudo apt update
sudo apt install -y rocm-dkms rocm-smi-lib rocm-opencl-runtime
sudo usermod -aG video $USER
sudo usermod -aG render $USER
echo 'export PATH=/opt/rocm/bin:$PATH' >> ~/.bashrc
echo 'export HSA_OVERRIDE_GFX_VERSION=11.0.0' >> ~/.bashrc
source ~/.bashrc

# 2. å®‰è£… Python + LLaMA-Factory
sudo apt install -y python3 python3-pip python3-venv
mkdir -p ~/amd-rocm-training && cd ~/amd-rocm-training
python3 -m venv .venv
source .venv/bin/activate
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .[torch,metrics]

# 3. åˆ›å»ºè„šæœ¬ï¼ˆå¤åˆ¶ä¸Šé¢çš„è„šæœ¬å†…å®¹ï¼‰
cd ~/amd-rocm-training
mkdir -p scripts config checkpoints logs
# ... åˆ›å»º training_config.yaml, start_training.sh, stop_training.sh, status.sh, monitor.sh

# 4. å¯åŠ¨è®­ç»ƒ
./scripts/start_training.sh

# 5. æŸ¥çœ‹çŠ¶æ€
./scripts/status.sh

# 6. åœæ­¢è®­ç»ƒ
./scripts/stop_training.sh
```

---

## âœ… æ ¸å¿ƒåŠŸèƒ½éªŒè¯

### 1. æœ‰ç©ºæ—¶å¯åŠ¨è®­ç»ƒ
```bash
# è¿›å…¥ WSL2
wsl -d Ubuntu-22.04

# å¯åŠ¨è®­ç»ƒ
cd ~/amd-rocm-training
./scripts/start_training.sh
```

### 2. æ²¡ç©ºæ—¶åœæ­¢è®­ç»ƒ
```bash
# ä¼˜é›…åœæ­¢ï¼ˆè‡ªåŠ¨ä¿å­˜ checkpointï¼‰
./scripts/stop_training.sh
```

### 3. æŸ¥çœ‹è®­ç»ƒçŠ¶æ€
```bash
# æŸ¥çœ‹è¿è¡ŒçŠ¶æ€ + GPU çŠ¶æ€ + æœ€æ–° checkpoint
./scripts/status.sh
```

### 4. æ¢å¤è®­ç»ƒ
```bash
# å†æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨ä»æœ€æ–° checkpoint æ¢å¤
./scripts/start_training.sh
```

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç¡®è®¤ M1A Pro+ å·²åˆ°æ‰‹å¹¶å¯è®¿é—®**
2. **æ‰§è¡Œä¸Šè¿° 6 æ­¥å®‰è£…æµç¨‹**
3. **å‡†å¤‡è®­ç»ƒæ•°æ®**ï¼ˆalpaca æ ¼å¼æˆ–è‡ªå®šä¹‰ï¼‰
4. **æµ‹è¯•å°è§„æ¨¡è’¸é¦**ï¼ˆå…ˆç”¨å°æ•°æ®é›†éªŒè¯æµç¨‹ï¼‰
5. **æ­£å¼è®­ç»ƒ**ï¼ˆæœ‰ç©ºæ—¶å¯åŠ¨ï¼Œæ²¡ç©ºæ—¶åœæ­¢ï¼‰
6. **æ¨¡å‹å¯¼å‡º + Claw é›†æˆæµ‹è¯•**

---

**ğŸ‰ å®Œæ•´æ–¹æ¡ˆå·²å®Œæˆï¼éšæ—¶å¯ä»¥å¼€å§‹å®æ–½ï¼** âš¡

**æœ‰ä»»ä½•é—®é¢˜éšæ—¶é—®æˆ‘ï¼** ğŸš€
