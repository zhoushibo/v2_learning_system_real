# OpenClaw APIé…ç½®ä¸å®ç°è¯´æ˜ä¹¦

**ç‰ˆæœ¬ï¼š** v1.0
**æ›´æ–°æ—¶é—´ï¼š** 2026-02-16
**é€‚ç”¨èŒƒå›´ï¼š** OpenClaw V2 å¼‚æ­¥æ¶æ„ï¼Œå¤šæ¨¡å‹è°ƒç”¨

---

## ğŸ“š **ç›®å½•**

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [APIé…ç½®æ±‡æ€»](#apié…ç½®æ±‡æ€»)
3. [APIè¯¦ç»†é…ç½®](#apiè¯¦ç»†é…ç½®)
4. [è°ƒç”¨ç¤ºä¾‹](#è°ƒç”¨ç¤ºä¾‹)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
6. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
7. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
8. [è´Ÿè½½å‡è¡¡ç­–ç•¥](#è´Ÿè½½å‡è¡¡ç­–ç•¥)
9. [ç›‘æ§ä¸å‘Šè­¦](#ç›‘æ§ä¸å‘Šè­¦)

---

## ğŸ“‹ **æ¦‚è¿°**

### é¡¹ç›®èƒŒæ™¯
OpenClaw V2 å¼‚æ­¥æ¶æ„é¡¹ç›®éœ€è¦ä½¿ç”¨å¤šä¸ªå¤§æ¨¡å‹APIï¼Œå®ç°æ™ºèƒ½è´Ÿè½½å‡è¡¡å’Œæ•…éšœé™çº§ã€‚æœ¬é¡¹ç›®æ•´åˆäº†5ä¸ªå…è´¹çš„APIæœåŠ¡ï¼Œé€šè¿‡ç»Ÿä¸€æ¥å£æä¾›ç¨³å®šçš„AIèƒ½åŠ›ã€‚

### æ ¸å¿ƒç›®æ ‡
- âœ… é«˜å¯ç”¨æ€§ï¼š16å¹¶å‘èƒ½åŠ›ï¼Œæ°¸ä¸å´©æºƒ
- âœ… é«˜æ€§èƒ½ï¼šæœ€å¿«1.03ç§’å“åº”ï¼Œæ™ºèƒ½è·¯ç”±
- âœ… é›¶æˆæœ¬ï¼šæ‰€æœ‰APIéƒ½æ˜¯å…è´¹çš„
- âœ… æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹
- âœ… å®¹é”™é™çº§ï¼š3å±‚å®¹é”™æœºåˆ¶ï¼Œè‡ªåŠ¨åˆ‡æ¢

### æ”¯æŒçš„æ¨¡å‹
| æ¨¡å‹ | é€Ÿåº¦ | ä¸Šä¸‹æ–‡ | å¹¶å‘ | RPMé™åˆ¶ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|------|---------|---------|
| æ™ºè°± | ğŸ¥‡ 1.03ç§’ | 200K | 1 | ? | æœ€å¿«å“åº” |
| æ··å…ƒ | ğŸ¥ˆ 1.20ç§’ | 256K | 5 | æ—  âš¡ | å¤§æ‰¹é‡ä»»åŠ¡ |
| è‹±ä¼Ÿè¾¾2 | ğŸ¥‰ 2.68ç§’ | 128K | 5 | 40/åˆ† | å¹³è¡¡å‹ |
| è‹±ä¼Ÿè¾¾1 | 7.17ç§’ | 128K | 5 | 40/åˆ† | å¤æ‚æ¨ç† |
| SiliconFlow | 0.10ç§’ | - | - | 5 RPM | Embeddings |

---

## ğŸ”§ **APIé…ç½®æ±‡æ€»**

### é…ç½®æ–‡ä»¶ä½ç½®
- JSONé…ç½®ï¼š`API_CONFIG_FINAL.json`
- å®Œæ•´æŠ¥å‘Šï¼š`API_SPEED_TEST_COMPLETE_REPORT.md`
- æµ‹è¯•è„šæœ¬ï¼š`api_quick_test.py`ã€`test_zhipu.py`

### å¿«é€Ÿé…ç½®åŠ è½½

```python
import json

# åŠ è½½APIé…ç½®
with open('API_CONFIG_FINAL.json', 'r', encoding='utf-8') as f:
    config = json.load(f)

api_configs = config['api_configs']

# è®¿é—®é…ç½®
zhipu_config = api_configs['zhipu']
hunyuan_config = api_configs['hunyuan']
nvidia1_config = api_configs['nvidia1']
nvidia2_config = api_configs['nvidia2']
siliconflow_config = api_configs['siliconflow']
```

---

## ğŸ“ **APIè¯¦ç»†é…ç½®**

### 1. æ™ºè°± glm4.7-flash

**åŸºæœ¬ä¿¡æ¯ï¼š**
```
Provider: æ™ºè°±
URL: https://open.bigmodel.cn/api/paas/v4/chat/completions
API KEY: c744282c23b74fa9bf7a2be68a8656b7.w4rIakRo0j4tWqpO
æ¨¡å‹: glm-4-flash
```

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- å¹³å‡å»¶è¿Ÿï¼š1.03ç§’ï¼ˆæœ€å¿«ï¼‰â­
- ä¸Šä¸‹æ–‡çª—å£ï¼š200,000 tokens
- RPMé™åˆ¶ï¼šå®˜æ–¹æœªæ˜ç¡®
- å¹¶å‘é™åˆ¶ï¼š1 âš ï¸ï¼ˆå…³é”®é™åˆ¶ï¼‰
- æ”¯æŒæ€è€ƒæ¨¡å¼ï¼šâœ…

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å®æ—¶äº¤äº’ï¼ˆæœ€å¿«å“åº”ï¼‰
- âœ… ä¸­å°ä¸Šä¸‹æ–‡ä»»åŠ¡ï¼ˆâ‰¤200Kï¼‰
- âœ… å•ä»»åŠ¡æˆ–ä½å¹¶å‘åœºæ™¯
- âš ï¸ ä¸é€‚åˆé«˜å¹¶å‘ä»»åŠ¡

**è°ƒç”¨ç¤ºä¾‹ï¼š**
```python
import requests

def call_zhipu_api(prompt):
    headers = {
        "Authorization": "Bearer c744282c23b74fa9bf7a2be68a8656b7.w4rIakRo0j4tWqpO",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "glm-4-flash",
        "messages": [{"role": "user", "content": prompt}],
        "thinking": {"type": "enabled"},  # å¯ç”¨æ€è€ƒæ¨¡å¼
        "max_tokens": 1024,
        "temperature": 0.7
    }

    response = requests.post(
        "https://open.bigmodel.cn/api/paas/v4/chat/completions",
        headers=headers,
        json=payload,
        timeout=30
    )

    result = response.json()
    content = result['choices'][0]['message']['content']
    thinking = result['choices'][0]['message'].get('thinking_content', '')

    return content, thinking
```

**æ³¨æ„äº‹é¡¹ï¼š**
âš ï¸ **é‡è¦ï¼š** å¹¶å‘é™åˆ¶åªæœ‰1ï¼Œå¿…é¡»ä½¿ç”¨é˜Ÿåˆ—æœºåˆ¶ç®¡ç†è¯·æ±‚ï¼Œé¿å…å¹¶å‘è¶…é™ã€‚

---

### 2. æ··å…ƒ hunyuan-lite

**åŸºæœ¬ä¿¡æ¯ï¼š**
```
Provider: è…¾è®¯æ··å…ƒ
URL: https://api.hunyuan.cloud.tencent.com/v1/chat/completions
API KEY: sk-7xGaNZwkW0CLZNeT8kZrJv2hiHpU47wzS8XVhOagKKjLyb2i
æ¨¡å‹: hunyuan-lite
```

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- å¹³å‡å»¶è¿Ÿï¼š1.20ç§’
- ä¸Šä¸‹æ–‡çª—å£ï¼š262,144 tokensï¼ˆæœ€å¤§ï¼‰â­
- RPMé™åˆ¶ï¼š**æ— é™åˆ¶** âš¡ï¼ˆæ€æ‰‹çº§ç‰¹æ€§ï¼‰
- å¹¶å‘é™åˆ¶ï¼š5ï¼ˆä¸»å­è´¦å·å…±äº«ï¼‰
- æ”¯æŒæ€è€ƒæ¨¡å¼ï¼šâŒ

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¤§æ‰¹é‡ä»»åŠ¡ï¼ˆæ— RPMé™åˆ¶ï¼‰â­
- âœ… é«˜å¹¶å‘åœºæ™¯ï¼ˆå¹¶å‘5ï¼‰
- âœ… è¶…å¤§ä¸Šä¸‹æ–‡ï¼ˆ>200Kï¼‰
- âœ… ä¸»è¦å·¥ä½œé©¬ï¼ˆæ¨è50%åˆ†é…ï¼‰

**è°ƒç”¨ç¤ºä¾‹ï¼š**
```python
import requests

def call_hunyuan_api(prompt):
    headers = {
        "Authorization": "Bearer sk-7xGaNZwkW0CLZNeT8kZrJv2hiHpU47wzS8XVhOagKKjLyb2i",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "hunyuan-lite",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1024,
        "temperature": 0.7,
        "extra_body": {
            "enable_enhancement": True
        }
    }

    response = requests.post(
        "https://api.hunyuan.cloud.tencent.com/v1/chat/completions",
        headers=headers,
        json=payload,
        timeout=30
    )

    result = response.json()
    content = result['choices'][0]['message']['content']

    return content
```

**æ³¨æ„äº‹é¡¹ï¼š**
âœ… **æ¨èï¼š** æ— RPMé™åˆ¶ï¼Œé€‚åˆä½œä¸ºä¸»è¦æ¨¡å‹ä½¿ç”¨ã€‚

---

### 3. è‹±ä¼Ÿè¾¾1 (ä¸»è´¦æˆ·) - z-ai/glm4.7

**åŸºæœ¬ä¿¡æ¯ï¼š**
```
Provider: NVIDIA (cherry-nvidia)
URL: https://integrate.api.nvidia.com/v1/chat/completions
API KEY: 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
æ¨¡å‹: z-ai/glm4.7
```

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- å¹³å‡å»¶è¿Ÿï¼š7.17ç§’ï¼ˆæœ€æ…¢ï¼Œä½†è´¨é‡é«˜ï¼‰
- ä¸Šä¸‹æ–‡çª—å£ï¼š128,000 tokens
- RPMé™åˆ¶ï¼š40/åˆ†é’Ÿ
- å¹¶å‘é™åˆ¶ï¼š5
- æ”¯æŒæ€è€ƒæ¨¡å¼ï¼šâœ…ï¼ˆæ·±åº¦æ€è€ƒï¼‰

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¤æ‚æ¨ç†ä»»åŠ¡
- âœ… éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»åŠ¡
- âœ… é•¿æ–‡æœ¬åˆ›ä½œ

**è°ƒç”¨ç¤ºä¾‹ï¼š**
```python
import requests

def call_nvidia1_api(prompt):
    headers = {
        "Authorization": "Bearer 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",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "z-ai/glm4.7",
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "temperature": 0.7,
        "max_tokens": 1000,
        "extra_body": {
            "chat_template_kwargs": {
                "enable_thinking": True,
                "clear_thinking": False
            }
        }
    }

    response = requests.post(
        "https://integrate.api.nvidia.com/v1/chat/completions",
        headers=headers,
        json=payload,
        timeout=120
    )

    result = response.json()
    content = result['choices'][0]['message']['content']

    # æå–æ€è€ƒå†…å®¹
    if hasattr(result['choices'][0]['message'], 'reasoning_content'):
        thinking = result['choices'][0]['message'].reasoning_content
    else:
        thinking = ""

    return content, thinking
```

**æ³¨æ„äº‹é¡¹ï¼š**
âš ï¸ **æ³¨æ„ï¼š** RPMé™åˆ¶40/åˆ†é’Ÿï¼Œéœ€è¦Token Bucketç®—æ³•æ§åˆ¶ã€‚

---

### 4. è‹±ä¼Ÿè¾¾2 (å¤‡ç”¨) - z-ai/glm4.7

**åŸºæœ¬ä¿¡æ¯ï¼š**
```
Provider: NVIDIA (cherry-nvidia)
URL: https://integrate.api.nvidia.com/v1/chat/completions
API KEY: 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
æ¨¡å‹: z-ai/glm4.7
```

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- å¹³å‡å»¶è¿Ÿï¼š2.68ç§’ï¼ˆæ¯”è‹±ä¼Ÿè¾¾1å¿«2.7å€ï¼‰
- ä¸Šä¸‹æ–‡çª—å£ï¼š128,000 tokens
- RPMé™åˆ¶ï¼š40/åˆ†é’Ÿ
- å¹¶å‘é™åˆ¶ï¼š5
- æ”¯æŒæ€è€ƒæ¨¡å¼ï¼šâœ…

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… é€Ÿåº¦ä¸è´¨é‡å¹³è¡¡
- âœ… è‹±ä¼Ÿè¾¾1å¤‡ç”¨
- âœ… æ ‡å‡†ä»»åŠ¡

**è°ƒç”¨ç¤ºä¾‹ï¼š**
```python
import requests

def call_nvidia2_api(prompt):
    headers = {
        "Authorization": "Bearer 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",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "z-ai/glm4.7",
        "messages": [{"role": "user", "content": prompt}],
        "stream": False,
        "temperature": 0.7,
        "max_tokens": 1000,
        "extra_body": {
            "chat_template_kwargs": {
                "enable_thinking": True,
                "clear_thinking": False
            }
        }
    }

    response = requests.post(
        "https://integrate.api.nvidia.com/v1/chat/completions",
        headers=headers,
        json=payload,
        timeout=120
    )

    result = response.json()
    content = result['choices'][0]['message']['content']

    return content
```

**æ³¨æ„äº‹é¡¹ï¼š**
âœ… **æ¨èï¼š** è‹±ä¼Ÿè¾¾2æ¯”è‹±ä¼Ÿè¾¾1ç¨³å®šï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨è‹±ä¼Ÿè¾¾2ã€‚

---

### 5. SiliconFlow (Embeddings)

**åŸºæœ¬ä¿¡æ¯ï¼š**
```
Provider: SiliconFlow
URL: https://api.siliconflow.cn/v1/embeddings
API KEY: sk-kvqpfofevcxloxexrrjovsjzpnwsvhpwrbxkwjydwbjyufjf
æ¨¡å‹: BAAI/bge-large-zh-v1.5
```

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- å¹³å‡å»¶è¿Ÿï¼š0.10ç§’ï¼ˆè¶…å¿«ï¼‰
- å‘é‡ç»´åº¦ï¼š1024
- RPMé™åˆ¶ï¼š5
- æ¯æ—¥é™åˆ¶ï¼š50ä¸‡ tokens

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… è®°å¿†æœç´¢ç³»ç»Ÿ
- âœ… è¯­ä¹‰æ£€ç´¢
- âœ… å‘é‡å­˜å‚¨

**è°ƒç”¨ç¤ºä¾‹ï¼š**
```python
import requests

def get_embedding(text):
    headers = {
        "Authorization": "Bearer sk-kvqpfofevcxloxexrrjovsjzpnwsvhpwrbxkwjydwbjyufjf",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "BAAI/bge-large-zh-v1.5",
        "input": text,
        "encoding_format": "float"
    }

    response = requests.post(
        "https://api.siliconflow.cn/v1/embeddings",
        headers=headers,
        json=payload,
        timeout=30
    )

    result = response.json()
    embedding = result['data'][0]['embedding']

    return embedding  # [1024ç»´å‘é‡]
```

**æ³¨æ„äº‹é¡¹ï¼š**
âš ï¸ **æ³¨æ„ï¼š** RPMé™åˆ¶5ï¼ŒEmbeddingsè°ƒç”¨åº”è¯¥ç¼“å­˜ç»“æœé¿å…é‡å¤è®¡ç®—ã€‚

---

## ğŸ’» **è°ƒç”¨ç¤ºä¾‹**

### ç»Ÿä¸€æ¥å£å°è£…

```python
import requests
import json
import time
from typing import Optional, Tuple

class MultiModelAPI:
    """å¤šæ¨¡å‹APIç»Ÿä¸€æ¥å£"""

    def __init__(self, config_path='API_CONFIG_FINAL.json'):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        self.api_configs = self.config['api_configs']

    def call_api(self, model_name: str, prompt: str, **kwargs) -> Tuple[str, dict]:
        """
        è°ƒç”¨æŒ‡å®šAPI

        Args:
            model_name: æ¨¡å‹åç§° (zhipu/hunyuan/nvidia1/nvidia2/siliconflow)
            prompt: ç”¨æˆ·æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            (response_content, usage_info)
        """
        config = self.api_configs[model_name]

        if config.get('type') == 'embeddings':
            return self._call_embedding_api(config, prompt, **kwargs)
        else:
            return self._call_chat_api(config, prompt, **kwargs)

    def _call_chat_api(self, config: dict, prompt: str, **kwargs) -> Tuple[str, dict]:
        """è°ƒç”¨èŠå¤©API"""
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": config['model'],
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
            "temperature": kwargs.get('temperature', 0.7),
            "max_tokens": kwargs.get('max_tokens', 1024)
        }

        # ç‰¹æ®Šå‚æ•°å¤„ç†
        if config['provider'] == 'nvidia' and config.get('enable_thinking'):
            payload['extra_body'] = {
                "chat_template_kwargs": {
                    "enable_thinking": True,
                    "clear_thinking": False
                }
            }
        elif config['provider'] == 'zhipu' and config.get('enable_thinking'):
            payload['thinking'] = {"type": "enabled"}
        elif config['provider'] == 'tencent':
            payload['extra_body'] = {
                "enable_enhancement": True
            }

        start_time = time.time()

        response = requests.post(
            config['url'],
            headers=headers,
            json=payload,
            timeout=kwargs.get('timeout', 60)
        )

        end_time = time.time()
        latency = end_time - start_time

        result = response.json()
        content = result['choices'][0]['message']['content']

        usage = {
            "latency": latency,
            "total_tokens": result.get('usage', {}).get('total_tokens', 0),
            "prompt_tokens": result.get('usage', {}).get('prompt_tokens', 0),
            "completion_tokens": result.get('usage', {}).get('completion_tokens', 0)
        }

        return content, usage

    def _call_embedding_api(self, config: dict, text: str, **kwargs) -> Tuple[list, dict]:
        """è°ƒç”¨Embedding API"""
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": config['model'],
            "input": text,
            "encoding_format": "float"
        }

        start_time = time.time()

        response = requests.post(
            config['url'],
            headers=headers,
            json=payload,
            timeout=kwargs.get('timeout', 30)
        )

        end_time = time.time()
        latency = end_time - start_time

        result = response.json()
        embedding = result['data'][0]['embedding']

        usage = {
            "latency": latency,
            "dimensions": len(embedding)
        }

        return embedding, usage


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    api = MultiModelAPI()

    # è°ƒç”¨æ™ºè°±
    content, usage = api.call_api("zhipu", "ä½ å¥½")
    print(f"æ™ºè°±å“åº”: {content}")
    print(f"è€—æ—¶: {usage['latency']:.2f}ç§’")

    # è°ƒç”¨æ··å…ƒ
    content, usage = api.call_api("hunyuan", "ä½ å¥½")
    print(f"æ··å…ƒå“åº”: {content}")
    print(f"è€—æ—¶: {usage['latency']:.2f}ç§’")
```

---

## ğŸ¯ **æœ€ä½³å®è·µ**

### 1. æ™ºèƒ½è·¯ç”±ç­–ç•¥

```python
def route_task(task_type: str, prompt_length: int, need_thinking: bool) -> str:
    """
    æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€ä¼˜æ¨¡å‹

    Args:
        task_type: ä»»åŠ¡ç±»å‹ (simple/complex/bulk/realtime)
        prompt_length: æç¤ºè¯é•¿åº¦
        need_thinking: æ˜¯å¦éœ€è¦æ€è€ƒæ¨¡å¼

    Returns:
        æ¨¡å‹åç§°
    """
    # å®æ—¶äº¤äº’ â†’ æ™ºè°±ï¼ˆæœ€å¿«ï¼‰
    if task_type == "realtime":
        return "zhipu"

    # å¤§æ‰¹é‡ä»»åŠ¡ â†’ æ··å…ƒï¼ˆæ— RPMé™åˆ¶ï¼‰
    elif task_type == "bulk":
        return "hunyuan"

    # å¤æ‚æ¨ç† â†’ è‹±ä¼Ÿè¾¾1ï¼ˆæ€è€ƒæ¨¡å¼æœ€æ·±ï¼‰
    elif need_thinking:
        return "nvidia1"

    # è¶…å¤§ä¸Šä¸‹æ–‡ï¼ˆ>200kï¼‰â†’ æ··å…ƒï¼ˆ256kï¼‰
    elif prompt_length > 200000:
        return "hunyuan"

    # å¤§ä¸Šä¸‹æ–‡ï¼ˆ128k-200kï¼‰â†’ æ™ºè°±ï¼ˆ200kï¼‰
    elif prompt_length > 128000:
        return "zhipu"

    # é»˜è®¤ â†’ è´Ÿè½½å‡è¡¡
    else:
        import random
        return weighted_random({
            "hunyuan": 0.50,    # 50%
            "nvidia1": 0.20,   # 20%
            "nvidia2": 0.20,   # 20%
            "zhipu": 0.10      # 10%
        })
```

### 2. å¹¶å‘æ§åˆ¶

```python
import asyncio
from asyncio import Semaphore

class ConcurrencyController:
    """å¹¶å‘æ§åˆ¶å™¨"""

    def __init__(self):
        # å¹¶å‘é™åˆ¶
        self.semaphores = {
            "zhipu": Semaphore(1),      # æ™ºè°±ï¼šåªæœ‰1å¹¶å‘
            "hunyuan": Semaphore(5),    # æ··å…ƒï¼š5å¹¶å‘
            "nvidia1": Semaphore(5),    # è‹±ä¼Ÿè¾¾1ï¼š5å¹¶å‘
            "nvidia2": Semaphore(5)     # è‹±ä¼Ÿè¾¾2ï¼š5å¹¶å‘
        }

    async def call_with_limit(self, model_name: str, api: MultiModelAPI, prompt: str):
        """å¸¦å¹¶å‘é™åˆ¶çš„è°ƒç”¨"""
        semaphore = self.semaphores[model_name]

        async with semaphore:
            # è°ƒç”¨API
            content, usage = await asyncio.to_thread(api.call_api, model_name, prompt)
            return content, usage


# ä½¿ç”¨ç¤ºä¾‹
async def concurrent_tasks():
    api = MultiModelAPI()
    controller = ConcurrencyController()

    tasks = [
        controller.call_with_limit("zhipu", api, prompt)
        for prompt in ["ä½ å¥½"] * 10
    ]

    results = await asyncio.gather(*tasks)
    return results
```

### 3. ç¼“å­˜ç­–ç•¥

```python
import hashlib
import json
from typing import Optional

class ResponseCache:
    """å“åº”ç¼“å­˜"""

    def __init__(self, redis_client=None):
        self.cache = {} if redis_client is None else redis_client
        self.ttl = 3600  # 1å°æ—¶

    def _get_cache_key(self, model: str, prompt: str):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key = f"{model}:{hashlib.md5(prompt.encode()).hexdigest()}"
        return key

    def get(self, model: str, prompt: str) -> Optional[str]:
        """è·å–ç¼“å­˜"""
        key = self._get_cache_key(model, prompt)

        if isinstance(self.cache, dict):
            return self.cache.get(key)
        else:
            # Redis
            value = self.cache.get(key)
            return json.loads(value) if value else None

    def set(self, model: str, prompt: str, response: str):
        """è®¾ç½®ç¼“å­˜"""
        key = self._get_cache_key(model, prompt)

        if isinstance(self.cache, dict):
            self.cache[key] = response
        else:
            # Redis
            self.cache.setex(key, self.ttl, json.dumps(response))


# ä½¿ç”¨ç¤ºä¾‹
def call_with_cache(api: MultiModelAPI, cache: ResponseCache, model: str, prompt: str):
    """å¸¦ç¼“å­˜çš„è°ƒç”¨"""
    # å…ˆæŸ¥ç¼“å­˜
    cached = cache.get(model, prompt)
    if cached:
        return cached

    # è°ƒç”¨API
    content, _ = api.call_api(model, prompt)

    # å†™å…¥ç¼“å­˜
    cache.set(model, prompt, content)

    return content
```

---

## âš ï¸ **é”™è¯¯å¤„ç†**

### 1. ç»Ÿä¸€é”™è¯¯å¤„ç†

```python
import requests
from typing import Tuple

class APIError(Exception):
    """APIé”™è¯¯åŸºç±»"""
    pass

class RateLimitError(APIError):
    """é€Ÿç‡é™åˆ¶é”™è¯¯"""
    pass

class TimeoutError(APIError):
    """è¶…æ—¶é”™è¯¯"""
    pass

class AuthenticationError(APIError):
    """è®¤è¯é”™è¯¯"""
    pass


def call_with_retry(
    api: MultiModelAPI,
    model: str,
    prompt: str,
    max_retries: int = 3,
    retry_delay: float = 2.0
) -> Tuple[str, dict]:
    """
    å¸¦é‡è¯•çš„APIè°ƒç”¨

    Args:
        api: APIå®ä¾‹
        model: æ¨¡å‹åç§°
        prompt: æç¤ºè¯
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

    Returns:
        (response_content, usage_info)

    Raises:
        APIError: æ‰€æœ‰é‡è¯•å¤±è´¥åæŠ›å‡º
    """
    last_error = None

    for attempt in range(max_retries):
        try:
            content, usage = api.call_api(model, prompt)
            return content, usage

        except requests.exceptions.Timeout as e:
            last_error = TimeoutError(f"APIè°ƒç”¨è¶…æ—¶: {e}")

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code

            if status_code == 429:
                # é€Ÿç‡é™åˆ¶
                wait_time = retry_delay * (2 ** attempt)
                last_error = RateLimitError(f"è§¦å‘é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•")
                time.sleep(wait_time)
                continue

            elif status_code == 401:
                # è®¤è¯å¤±è´¥
                raise AuthenticationError(f"APIè®¤è¯å¤±è´¥: {e.response.text}")

            else:
                # å…¶ä»–HTTPé”™è¯¯
                last_error = APIError(f"HTTPé”™è¯¯ {status_code}: {e.response.text}")

        except Exception as e:
            last_error = APIError(f"æœªçŸ¥é”™è¯¯: {e}")

        # ç­‰å¾…é‡è¯•
        if attempt < max_retries - 1:
            wait_time = retry_delay * (2 ** attempt)
            time.sleep(wait_time)

    # æ‰€æœ‰é‡è¯•å¤±è´¥
    raise last_error
```

### 2. æ•…éšœé™çº§

```python
def call_with_fallback(
    api: MultiModelAPI,
    models: list,
    prompt: str
) -> Tuple[str, str, dict]:
    """
    æ•…éšœé™çº§ï¼šä¾æ¬¡å°è¯•å¤šä¸ªæ¨¡å‹

    Args:
        api: APIå®ä¾‹
        models: æ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        prompt: æç¤ºè¯

    Returns:
        (response_content, model_used, usage_info)
    """
    last_error = None

    for model in models:
        try:
            content, usage = call_with_retry(api, model, prompt)
            return content, model, usage

        except APIError as e:
            last_error = e
            print(f"[WARN] {model} å¤±è´¥: {e}")
            continue

    # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥
    raise APIError(f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥: {last_error}")


# ä½¿ç”¨ç¤ºä¾‹
def smart_call(api: MultiModelAPI, prompt: str):
    """æ™ºèƒ½è°ƒç”¨ï¼šè‡ªåŠ¨é™çº§"""

    # åœºæ™¯1ï¼šå®æ—¶ä»»åŠ¡
    models = ["zhipu", "hunyuan", "nvidia2", "nvidia1"]
    content, model_used, usage = call_with_fallback(api, models, prompt)

    print(f"ä½¿ç”¨æ¨¡å‹: {model_used}")
    print(f"è€—æ—¶: {usage['latency']:.2f}ç§’")

    return content
```

---

## âš¡ **æ€§èƒ½ä¼˜åŒ–**

### 1. æ‰¹é‡è¯·æ±‚ä¼˜åŒ–

```python
from typing import List
import asyncio

async def batch_call_api(
    api: MultiModelAPI,
    model: str,
    prompts: List[str],
    max_concurrent: int = 5
) -> List[Tuple[str, dict]]:
    """
    æ‰¹é‡è°ƒç”¨API

    Args:
        api: APIå®ä¾‹
        model: æ¨¡å‹åç§°
        prompts: æç¤ºè¯åˆ—è¡¨
        max_concurrent: æœ€å¤§å¹¶å‘æ•°

    Returns:
        ç»“æœåˆ—è¡¨
    """
    semaphore = Semaphore(max_concurrent)

    async def single_call(prompt: str):
        async with semaphore:
            content, usage = await asyncio.to_thread(api.call_api, model, prompt)
            return (content, usage)

    tasks = [single_call(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)

    return results


# ä½¿ç”¨ç¤ºä¾‹
async def run_batch():
    api = MultiModelAPI()

    prompts = ["ä½ å¥½"] * 10
    results = await batch_call_api(api, "zhipu", prompts, max_concurrent=1)

    for content, usage in results:
        print(f"å“åº”: {content}, è€—æ—¶: {usage['latency']:.2f}ç§’")
```

### 2. å“åº”ç¼“å­˜

è§ä¸Šæ–‡ `ResponseCache` ç±»ã€‚

### 3. æµå¼å“åº”ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰

```python
def stream_response(api: MultiModelAPI, model: str, prompt: str):
    """æµå¼å“åº”"""
    config = api.api_configs[model]

    headers = {
        "Authorization": f"Bearer {config['api_key']}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": config['model'],
        "messages": [{"role": "user", "content": prompt}],
        "stream": True,
        "max_tokens": 1024
    }

    response = requests.post(
        config['url'],
        headers=headers,
        json=payload,
        stream=True,
        timeout=60
    )

    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data = json.loads(line[6:])
                if data['choices'][0]['finish_reason'] is None:
                    content = data['choices'][0]['delta']['content']
                    yield content
```

---

## ğŸ® **è´Ÿè½½å‡è¡¡ç­–ç•¥**

```python
import random

def weighted_random(weights: dict) -> str:
    """
    åŠ æƒéšæœºé€‰æ‹©

    Args:
        weights: {model_name: weight}

    Returns:
        é€‰ä¸­çš„æ¨¡å‹åç§°
    """
    models = list(weights.keys())
    probabilities = list(weights.values())

    return random.choices(models, weights=probabilities)[0]


# è´Ÿè½½å‡è¡¡é…ç½®
LOAD_BALANCING_CONFIG = {
    "default": {
        "hunyuan": 0.50,
        "nvidia1": 0.20,
        "nvidia2": 0.20,
        "zhipu": 0.10
    },
    "bulk": {
        "hunyuan": 1.0
    },
    "realtime": {
        "zhipu": 1.0
    },
    "complex": {
        "nvidia1": 0.6,
        "nvidia2": 0.4
    }
}


def load_balance(task_type: str = "default") -> str:
    """è´Ÿè½½å‡è¡¡"""
    config = LOAD_BALANCING_CONFIG.get(task_type, LOAD_BALANCING_CONFIG["default"])
    return weighted_random(config)
```

---

## ğŸ“Š **ç›‘æ§ä¸å‘Šè­¦**

### 1. æ€§èƒ½ç›‘æ§

```python
import time
from collections import defaultdict

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§"""

    def __init__(self):
        self.stats = defaultdict(lambda: {
            "total_calls": 0,
            "total_latency": 0,
            "success_count": 0,
            "failure_count": 0
        })

    def record_call(self, model: str, latency: float, success: bool):
        """è®°å½•è°ƒç”¨"""
        stats = self.stats[model]
        stats["total_calls"] += 1
        stats["total_latency"] += latency

        if success:
            stats["success_count"] += 1
        else:
            stats["failure_count"] += 1

    def get_stats(self, model: str) -> dict:
        """è·å–ç»Ÿè®¡"""
        stats = self.stats[model]

        if stats["total_calls"] == 0:
            return stats.copy()

        return {
            "total_calls": stats["total_calls"],
            "avg_latency": stats["total_latency"] / stats["total_calls"],
            "success_rate": stats["success_count"] / stats["total_calls"],
            "failure_count": stats["failure_count"]
        }


# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor()

# è®°å½•è°ƒç”¨
monitor.record_call("zhipu", 1.03, True)
monitor.record_call("hunyuan", 1.20, True)

# è·å–ç»Ÿè®¡
print(monitor.get_stats("zhipu"))
```

### 2. å¥åº·æ£€æŸ¥

```python
def health_check(api: MultiModelAPI) -> dict:
    """å¥åº·æ£€æŸ¥"""
    results = {}

    for model_name in api.api_configs.keys():
        if model_name == "siliconflow":
            continue

        try:
            # ç®€å•æµ‹è¯•è°ƒç”¨
            content, usage = api.call_api(model_name, "æµ‹è¯•")

            results[model_name] = {
                "status": "healthy",
                "latency": usage["latency"],
                "last_check": time.time()
            }

        except Exception as e:
            results[model_name] = {
                "status": "unhealthy",
                "error": str(e),
                "last_check": time.time()
            }

    return results
```

---

## ğŸ“ **æ€»ç»“**

### å…³é”®è¦ç‚¹

1. **æ™ºè°±**ï¼šé€Ÿåº¦æœ€å¿«ï¼ˆ1.03ç§’ï¼‰ï¼Œä½†å¹¶å‘åªæœ‰1ï¼Œé€‚åˆå•ä»»åŠ¡
2. **æ··å…ƒ**ï¼šæ— RPMé™åˆ¶ï¼Œå¹¶å‘5ï¼Œé«˜å¹¶å‘åœºæ™¯æœ€ä¼˜
3. **è‹±ä¼Ÿè¾¾**ï¼šæ€è€ƒæ¨¡å¼æœ€æ·±ï¼Œé€‚åˆå¤æ‚æ¨ç†
4. **SiliconFlow**ï¼šEmbeddingsä¸“ç”¨ï¼Œ0.10ç§’è¶…å¿«

### æœ€ä½³å®è·µ

- âœ… ä½¿ç”¨æ™ºèƒ½è·¯ç”±ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ¨¡å‹
- âœ… å®ç°é‡è¯•å’Œé™çº§æœºåˆ¶ï¼Œä¿è¯é«˜å¯ç”¨
- âœ… ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨
- âœ… æ§åˆ¶å¹¶å‘ï¼Œé¿å…è§¦å‘é™åˆ¶
- âœ… ç›‘æ§æ€§èƒ½ï¼ŒåŠæ—¶å‘ç°é—®é¢˜

### ä¸‹ä¸€æ­¥

- å®æ–½OpenClaw V2 MVPé›†æˆ
- å¼€å‘MultiModelRateLimiter
- å¼€å‘TaskClassifier
- å®ç°è´Ÿè½½å‡è¡¡å’Œæ™ºèƒ½è·¯ç”±

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** v1.0
**æœ€åæ›´æ–°ï¼š** 2026-02-16
**ç»´æŠ¤è€…ï¼š** Claw
