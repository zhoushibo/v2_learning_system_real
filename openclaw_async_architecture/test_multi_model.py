"""æµ‹è¯•å¤šæ¨¡å‹ç­–ç•¥é›†æˆ"""
import requests
import time
import json

V2_GATEWAY = "http://127.0.0.1:8000"


def test_multi_model_integration():
    """æµ‹è¯•å¤šæ¨¡å‹ç­–ç•¥å®Œæ•´æµç¨‹"""
    print("="*70)
    print("OpenClaw V2 å¤šæ¨¡å‹ç­–ç•¥é›†æˆæµ‹è¯•")
    print("="*70)

    test_cases = [
        {
            "prompt": "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±",
            "type": "simple",
            "expected_model": "hunyuan or zhipu"
        },
        {
            "prompt": "ç°åœ¨é©¬ä¸Šç¿»è¯‘è¿™å¥è¯",
            "type": "realtime",
            "expected_model": "zhipu (æœ€å¿«)"
        },
        {
            "prompt": "æ·±å…¥åˆ†æäººå·¥æ™ºèƒ½å¯¹ç¤¾ä¼šçš„å½±å“",
            "type": "complex",
            "expected_model": "nvidia1 (æ€è€ƒæ¨¡å¼)"
        },
        {
            "prompt": "æ‰¹é‡ç¿»è¯‘è¿™100ç¯‡æ–‡ç« ",
            "type": "bulk",
            "expected_model": "hunyuan (æ— RPMé™åˆ¶)"
        }
    ]

    results = []

    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*70}")
        print(f"æµ‹è¯• {i}/{len(test_cases)}: {test_case['type']}")
        print(f"{'='*70}")
        print(f"æç¤ºè¯: {test_case['prompt']}")
        print(f"é¢„æœŸæ¨¡å‹: {test_case['expected_model']}")

        # æäº¤ä»»åŠ¡
        start_time = time.time()

        response = requests.post(
            f"{V2_GATEWAY}/tasks",
            json={"content": test_case['prompt']},
            timeout=5
        )

        submit_time = time.time() - start_time

        if response.status_code != 200:
            print(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥")
            continue

        task_id = response.json()['task_id']
        print(f"âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task_id}")
        print(f"â±ï¸  æäº¤æ—¶é—´: {submit_time*1000:.2f}ms")

        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        print(f"\nâ³ ç­‰å¾…ä»»åŠ¡æ‰§è¡Œ...")
        task_result = None
        for j in range(30):
            time.sleep(1)

            response = requests.get(
                f"{V2_GATEWAY}/tasks/{task_id}",
                timeout=5
            )

            if response.status_code == 200:
                data = response.json()
                if data['status'] == "completed":
                    task_result = data
                    break
                elif data['status'] == "failed":
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {data['error']}")
                    break

        if task_result:
            print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼")
            print(f"çŠ¶æ€: {task_result['status']}")
            print(f"ç»“æœ: {task_result['result'][:100]}...")

            # æå–æ¨¡å‹ä¿¡æ¯
            if 'metadata' in task_result and task_result['metadata']:
                model = task_result['metadata'].get('model', 'unknown')
                latency = task_result['metadata'].get('latency', 0)
                print(f"\nğŸ“Š æ‰§è¡Œè¯¦æƒ…:")
                print(f"  ğŸ¤– å®é™…ä½¿ç”¨æ¨¡å‹: {model}")
                print(f"  â±ï¸  è€—æ—¶: {latency:.2f}ç§’")

                results.append({
                    "test": test_case['type'],
                    "prompt": test_case['prompt'],
                    "model": model,
                    "latency": latency,
                    "expected": test_case['expected_model']
                })
            else:
                print(f"\nâš ï¸  æ²¡æœ‰å…ƒæ•°æ®ä¿¡æ¯")

    # æ±‡æ€»ç»Ÿè®¡
    print(f"\n{'='*70}")
    print("æµ‹è¯•æ±‡æ€»")
    print(f"{'='*70}")

    models_used = set(r['model'] for r in results if r['model'] != 'unknown')
    avg_latency = sum(r['latency'] for r in results) / len(results) if results else 0

    print(f"âœ… æˆåŠŸå®Œæˆ: {len(results)}/{len(test_cases)}")
    print(f"âœ… ä½¿ç”¨çš„æ¨¡å‹: {', '.join(models_used)}")
    print(f"â±ï¸  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ç§’")
    print(f"\nè¯¦ç»†ç»“æœ:")
    print(f"{'æµ‹è¯•ç±»å‹':<15} {'æ¨¡å‹':<20} {'è€—æ—¶(ç§’)':<10} {'é¢„æœŸ'}")
    print(f"{'-'*70}")
    for r in results:
        print(f"{r['test']:<15} {r['model']:<20} {r['latency']:<10.2f} {r['expected']}")

    return len(results) == len(test_cases)


def test_load_balancer_direct():
    """ç›´æ¥æµ‹è¯•LoadBalancer"""
    print(f"\n{'='*70}")
    print("LoadBalancerç›´æ¥æµ‹è¯•")
    print(f"{'='*70}")

    import sys
    import os
    sys.path.insert(0, r'C:\Users\10952\.openclaw\workspace\openclaw_async_architecture\mvp\src')

    from common.load_balancer import get_load_balancer

    balancer = get_load_balancer()

    print(f"\næµ‹è¯•ä¸åŒç±»å‹ä»»åŠ¡:")

    test_prompts = [
        "å¿«é€Ÿå›ç­”ï¼šä½ å¥½",
        "åˆ†æï¼šæ·±åº¦æ€è€ƒä¸€ä¸ªé—®é¢˜",
        "æ‰¹é‡ï¼šå¤„ç†å¾ˆå¤šä»»åŠ¡",
        "ç°åœ¨ï¼šç«‹å³ç¿»è¯‘"
    ]

    for prompt in test_prompts:
        print(f"\n{'--'*35}")
        print(f"æç¤ºè¯: {prompt}")

        result = balancer.call_api(prompt)

        if result['success']:
            print(f"âœ… æˆåŠŸ")
            print(f"  æ¨¡å‹: {result['model']}")
            print(f"  è€—æ—¶: {result['latency']:.2f}ç§’")
            print(f"  å†…å®¹: {result['content'][:80]}...")
        else:
            print(f"âŒ å¤±è´¥: {result.get('error')}")

    # ç»Ÿè®¡
    print(f"\n{'--'*35}")
    print(f"ç»Ÿè®¡ä¿¡æ¯:")
    print(json.dumps(balancer.get_stats(), indent=2, ensure_ascii=False))


if __name__ == "__main__":
    test_mode = input("é€‰æ‹©æµ‹è¯•æ¨¡å¼ (1=å®Œæ•´MVPæµ‹è¯•, 2=LoadBalancerç›´æ¥æµ‹è¯•): ").strip()

    if test_mode == "2":
        test_load_balancer_direct()
    else:
        success = test_multi_model_integration()

        print(f"\n{'='*70}")
        if success:
            print("âœ… å¤šæ¨¡å‹ç­–ç•¥é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print(f"{'='*70}")
