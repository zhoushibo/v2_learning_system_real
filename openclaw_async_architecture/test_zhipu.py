#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºè°± glm4.7-flash é€Ÿåº¦æµ‹è¯•è„šæœ¬
"""

import time
import requests
from datetime import datetime
import sys
import io

# ä¿®å¤Windows GBKç¼–ç é—®é¢˜
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# æ™ºè°±é…ç½®
ZHIPU_CONFIG = {
    "name": "æ™ºè°± glm4.7-flash",
    "provider": "zhipu",
    "url": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
    "api_key": "c744282c23b74fa9bf7a2be68a8656b7.w4rIakRo0j4tWqpO",
    "model": "glm-4-flash",
    "context_window": 200000,
    "max_concurrent": 1,
    "note": "200Kä¸Šä¸‹æ–‡ï¼Œæ”¯æŒ thinkers æ¨¡å¼"
}

def test_zhipu(prompt):
    """æµ‹è¯•æ™ºè°± API"""
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•: {ZHIPU_CONFIG['name']}")
    print(f"æç¤ºè¯: {prompt}")
    print(f"{'='*70}")

    headers = {
        "Authorization": f"Bearer {ZHIPU_CONFIG['api_key']}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": ZHIPU_CONFIG['model'],
        "messages": [{"role": "user", "content": prompt}],
        "thinking": {"type": "enabled"},
        "max_tokens": 1024,
        "temperature": 0.7,
        "stream": False
    }

    try:
        start_time = time.time()

        response = requests.post(
            ZHIPU_CONFIG['url'],
            headers=headers,
            json=payload,
            timeout=60
        )

        end_time = time.time()
        latency = end_time - start_time

        response.raise_for_status()
        result = response.json()

        # æå–ä¿¡æ¯
        try:
            content = result['choices'][0]['message']['content']
            thinking_content = result['choices'][0]['message'].get('thinking_content', '')

        except (KeyError, IndexError, TypeError):
            print(f"  âœ— è§£æå“åº”å¤±è´¥")
            print(f"  å“åº”ç»“æ„: {result}")
            return None

        if content is None or content == "":
            print(f"  âœ— å“åº”å†…å®¹ä¸ºç©º")
            return None

        total_tokens = result.get('usage', {}).get('total_tokens', 0)

        print(f"  [OK] å»¶è¿Ÿ: {latency:.2f}ç§’")
        print(f"  [OK] Tokens: {total_tokens}")
        print(f"  [OK] å“åº”é•¿åº¦: {len(content)}å­—ç¬¦")

        if thinking_content:
            print(f"  [OK] æ€è€ƒå†…å®¹: ({len(thinking_content)}å­—ç¬¦)")
            print(f"  æ€è€ƒé¢„è§ˆ: {thinking_content[:100]}...")

        print(f"  [OK] å“åº”: {content[:100]}{'...' if len(content) > 100 else ''}")

        return {
            "name": ZHIPU_CONFIG['name'],
            "provider": ZHIPU_CONFIG['provider'],
            "model": ZHIPU_CONFIG['model'],
            "latency": latency,
            "tokens": total_tokens,
            "response_length": len(content),
            "response": content,
            "has_thinking": bool(thinking_content)
        }

    except requests.exceptions.Timeout:
        print(f"  âœ— è¶…æ—¶ (>60ç§’)")
        return None
    except requests.exceptions.RequestException as e:
        print(f"  âœ— é”™è¯¯: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{'='*70}")
    print(f"æ™ºè°± glm4.7-flash é€Ÿåº¦æµ‹è¯•")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*70}")

    # æµ‹è¯•ç®€å•æç¤ºè¯
    print(f"\n\n{'#'*70}")
    print(f"# æµ‹è¯•1: ç®€å•æç¤ºè¯ ('ä½ å¥½')")
    print(f"{'#'*70}\n")

    result1 = test_zhipu("ä½ å¥½")

    # ç­‰å¾…ä¸€ä¸‹ï¼ˆç¡®ä¿ä¸ä¼šè§¦å‘å¹¶å‘é™åˆ¶ï¼‰
    time.sleep(2)

    # æµ‹è¯•å¤æ‚æç¤ºè¯
    print(f"\n\n{'#'*70}")
    print(f"# æµ‹è¯•2: å¤æ‚æç¤ºè¯ (çŸ­æ•…äº‹)")
    print(f"{'#'*70}\n")

    result2 = test_zhipu("ç”¨50å­—å†™ä¸€ä¸ªå…³äºAIçš„å°æ•…äº‹")

    # æ±‡æ€»
    print(f"\n\n{'='*70}")
    print(f"ğŸ“Š æ™ºè°± glm4.7-flash æµ‹è¯•ç»“æœ")
    print(f"{'='*70}\n")

    if result1 and result2:
        avg_latency = (result1['latency'] + result2['latency']) / 2

        print(f"æµ‹è¯•1 (ç®€å•): {result1['latency']:.2f}ç§’, {result1['tokens']} tokens")
        print(f"æµ‹è¯•2 (å¤æ‚): {result2['latency']:.2f}ç§’, {result2['tokens']} tokens")
        print(f"\n{'-'*70}")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ç§’")
        print(f"æ€è€ƒæ¨¡å¼: {'âœ“ å·²å¯ç”¨' if (result1['has_thinking'] or result2['has_thinking']) else 'âœ— æœªå¯ç”¨'}")
        print(f"{'-'*70}")

        # ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”ï¼ˆåŸºäºä¹‹å‰æµ‹è¯•ç»“æœï¼‰
        print(f"\nğŸ† é€Ÿåº¦å¯¹æ¯”ï¼ˆåŸºäºä¹‹å‰æµ‹è¯•ç»“æœï¼‰")
        print(f"{'-'*70}")
        print(f"{'æ’å':<6} {'æ¨¡å‹':<25} {'å¹³å‡å»¶è¿Ÿ(ç§’)':<15} {'ä¸Šä¸‹æ–‡':<10}")
        print(f"{'-'*70}")

        # æ’åºï¼ˆæ›´æ–°åçš„æ’åï¼‰
        all_models = [
            {"name": "æ··å…ƒ (è…¾è®¯)", "latency": 1.20, "context": "256k"},
            {"name": "è‹±ä¼Ÿè¾¾2", "latency": 2.68, "context": "128k"},
            {"name": "æ™ºè°± glm4.7-flash", "latency": avg_latency, "context": "200k"},
            {"name": "è‹±ä¼Ÿè¾¾1", "latency": 7.17, "context": "128k"}
        ]

        sorted_models = sorted(all_models, key=lambda x: x['latency'])

        for rank, model in enumerate(sorted_models, 1):
            medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰"
            marker = " â­" if "æ™ºè°±" in model['name'] else ""
            print(f"{rank:<6} {medal} {model['name']:<25} {model['latency']:<15.2f} {model['context']:<10}{marker}")

    else:
        print("âœ— æµ‹è¯•å¤±è´¥")

    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
